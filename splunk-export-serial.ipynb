{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74848c1b-173b-45e9-bee3-4f089d168f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "import dateutil.parser\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import portalocker\n",
    "import logging\n",
    "#from splunk_http_event_collector import http_event_collector\n",
    "import timeout_decorator\n",
    "import configparser\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "#if len(sys.argv) < 2:\n",
    "#    print \"Pass the name of a config file as argument 1\"\n",
    "#    exit(1)\n",
    "\n",
    "#if not os.path.exists(sys.argv[1]):\n",
    "#    print \"Cannot find the configuration file: \"+sys.argv[1]\n",
    "#    exit(1)\n",
    "config_file='export1.conf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883934aa-dc69-4731-8957-354ba55898ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers[0].stream = sys.stdout\n",
    "logger.setLevel(logging.INFO) # DEBUG,INFO,WARNING,ERROR,CRITICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d4980-2069-406d-87ad-efcbd6375c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a6e91a-c606-421c-8721-82f51ee3b52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-29 18:12:07 root         INFO     connect-start\n",
      "09-29 18:12:07 root         INFO     connect-successful\n",
      "09-29 18:12:07 root         INFO     connect-end\n",
      "09-29 18:12:07 root         INFO     explode_date_range-start\n",
      "09-29 18:12:07 root         INFO     explode_date_range-end\n",
      "09-29 18:12:07 root         INFO     write_search_partitions-start\n",
      "09-29 18:12:07 root         INFO     write_search_partitions-end\n",
      "09-29 18:12:07 root         INFO     dispatch_searches-start\n",
      "09-29 18:12:07 root         INFO     build_search_string-start\n",
      "09-29 18:12:07 root         INFO     build_search_string-end\n",
      "09-29 18:12:07 root         INFO     connect-start\n",
      "09-29 18:12:07 root         INFO     connect-successful\n",
      "09-29 18:12:07 root         INFO     connect-end\n",
      "09-29 18:12:07 root         INFO     search_export-start\n",
      "09-29 18:12:07 root         INFO     search_export-end\n",
      "09-29 18:12:07 root         INFO     write_results-start\n",
      "09-29 18:12:08 root         INFO     write_results-end\n",
      "09-29 18:12:08 root         INFO     build_search_string-start\n",
      "09-29 18:12:08 root         INFO     build_search_string-end\n",
      "09-29 18:12:08 root         INFO     connect-start\n",
      "09-29 18:12:08 root         INFO     connect-successful\n",
      "09-29 18:12:08 root         INFO     connect-end\n",
      "09-29 18:12:08 root         INFO     search_export-start\n",
      "09-29 18:12:08 root         INFO     search_export-end\n",
      "09-29 18:12:08 root         INFO     write_results-start\n",
      "09-29 18:12:10 root         INFO     write_results-end\n",
      "09-29 18:12:10 root         INFO     build_search_string-start\n",
      "09-29 18:12:10 root         INFO     build_search_string-end\n",
      "09-29 18:12:10 root         INFO     connect-start\n",
      "09-29 18:12:10 root         INFO     connect-successful\n",
      "09-29 18:12:10 root         INFO     connect-end\n",
      "09-29 18:12:10 root         INFO     search_export-start\n",
      "09-29 18:12:10 root         INFO     search_export-end\n",
      "09-29 18:12:10 root         INFO     write_results-start\n",
      "09-29 18:12:11 root         INFO     write_results-end\n",
      "09-29 18:12:11 root         INFO     build_search_string-start\n",
      "09-29 18:12:11 root         INFO     build_search_string-end\n",
      "09-29 18:12:11 root         INFO     connect-start\n",
      "09-29 18:12:11 root         INFO     connect-successful\n",
      "09-29 18:12:11 root         INFO     connect-end\n",
      "09-29 18:12:11 root         INFO     search_export-start\n",
      "09-29 18:12:11 root         INFO     search_export-end\n",
      "09-29 18:12:11 root         INFO     write_results-start\n",
      "09-29 18:12:12 root         INFO     write_results-end\n",
      "09-29 18:12:12 root         INFO     build_search_string-start\n",
      "09-29 18:12:12 root         INFO     build_search_string-end\n",
      "09-29 18:12:12 root         INFO     connect-start\n",
      "09-29 18:12:12 root         INFO     connect-successful\n",
      "09-29 18:12:12 root         INFO     connect-end\n",
      "09-29 18:12:12 root         INFO     search_export-start\n",
      "09-29 18:12:12 root         INFO     search_export-end\n",
      "09-29 18:12:12 root         INFO     write_results-start\n",
      "09-29 18:12:13 root         INFO     write_results-end\n",
      "09-29 18:12:13 root         INFO     build_search_string-start\n",
      "09-29 18:12:13 root         INFO     build_search_string-end\n",
      "09-29 18:12:13 root         INFO     connect-start\n",
      "09-29 18:12:13 root         INFO     connect-successful\n",
      "09-29 18:12:13 root         INFO     connect-end\n",
      "09-29 18:12:13 root         INFO     search_export-start\n",
      "09-29 18:12:13 root         INFO     search_export-end\n",
      "09-29 18:12:13 root         INFO     write_results-start\n",
      "09-29 18:12:15 root         INFO     write_results-end\n",
      "09-29 18:12:15 root         INFO     build_search_string-start\n",
      "09-29 18:12:15 root         INFO     build_search_string-end\n",
      "09-29 18:12:15 root         INFO     connect-start\n",
      "09-29 18:12:15 root         INFO     connect-successful\n",
      "09-29 18:12:15 root         INFO     connect-end\n",
      "09-29 18:12:15 root         INFO     search_export-start\n",
      "09-29 18:12:15 root         INFO     search_export-end\n",
      "09-29 18:12:15 root         INFO     write_results-start\n",
      "09-29 18:12:16 root         INFO     write_results-end\n",
      "09-29 18:12:16 root         INFO     build_search_string-start\n",
      "09-29 18:12:16 root         INFO     build_search_string-end\n",
      "09-29 18:12:16 root         INFO     connect-start\n",
      "09-29 18:12:16 root         INFO     connect-successful\n",
      "09-29 18:12:16 root         INFO     connect-end\n",
      "09-29 18:12:16 root         INFO     search_export-start\n",
      "09-29 18:12:16 root         INFO     search_export-end\n",
      "09-29 18:12:16 root         INFO     write_results-start\n",
      "09-29 18:12:17 root         INFO     write_results-end\n",
      "09-29 18:12:17 root         INFO     build_search_string-start\n",
      "09-29 18:12:17 root         INFO     build_search_string-end\n",
      "09-29 18:12:17 root         INFO     connect-start\n",
      "09-29 18:12:17 root         INFO     connect-successful\n",
      "09-29 18:12:17 root         INFO     connect-end\n",
      "09-29 18:12:17 root         INFO     search_export-start\n",
      "09-29 18:12:17 root         INFO     search_export-end\n",
      "09-29 18:12:17 root         INFO     write_results-start\n",
      "09-29 18:12:18 root         INFO     write_results-end\n",
      "09-29 18:12:18 root         INFO     build_search_string-start\n",
      "09-29 18:12:18 root         INFO     build_search_string-end\n",
      "09-29 18:12:18 root         INFO     connect-start\n",
      "09-29 18:12:18 root         INFO     connect-successful\n",
      "09-29 18:12:18 root         INFO     connect-end\n",
      "09-29 18:12:18 root         INFO     search_export-start\n",
      "09-29 18:12:18 root         INFO     search_export-end\n",
      "09-29 18:12:18 root         INFO     write_results-start\n",
      "09-29 18:12:19 root         INFO     write_results-end\n",
      "09-29 18:12:19 root         INFO     build_search_string-start\n",
      "09-29 18:12:19 root         INFO     build_search_string-end\n",
      "09-29 18:12:19 root         INFO     connect-start\n",
      "09-29 18:12:19 root         INFO     connect-successful\n",
      "09-29 18:12:19 root         INFO     connect-end\n",
      "09-29 18:12:19 root         INFO     search_export-start\n",
      "09-29 18:12:19 root         INFO     search_export-end\n",
      "09-29 18:12:19 root         INFO     write_results-start\n",
      "09-29 18:12:20 root         INFO     write_results-end\n",
      "09-29 18:12:20 root         INFO     build_search_string-start\n",
      "09-29 18:12:20 root         INFO     build_search_string-end\n",
      "09-29 18:12:20 root         INFO     connect-start\n",
      "09-29 18:12:20 root         INFO     connect-successful\n",
      "09-29 18:12:20 root         INFO     connect-end\n",
      "09-29 18:12:20 root         INFO     search_export-start\n",
      "09-29 18:12:20 root         INFO     search_export-end\n",
      "09-29 18:12:20 root         INFO     write_results-start\n",
      "09-29 18:12:21 root         INFO     write_results-end\n",
      "09-29 18:12:21 root         INFO     dispatch_searches-end\n"
     ]
    }
   ],
   "source": [
    "def load_config():\n",
    "    global config\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "def create_output_dir():\n",
    "    path=config.get('export', 'directory')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def build_search_string(partition_in):\n",
    "    logging.info('build_search_string-start')\n",
    "    logging.debug('indexes: %s',config.get('search', 'indexes'))\n",
    "    logging.debug('extra: %s',config.get('search', 'extra'))\n",
    "    s='search index='+partition_in[\"index\"]\n",
    "    #s+=' earliest='+partition_in[\"earliest\"]\n",
    "    #s+=' latest='+partition_in[\"latest\"]\n",
    "    s+=' '+config.get('search', 'extra')\n",
    "    \n",
    "    logging.debug('s: %s',s)\n",
    "    logging.info('build_search_string-end')\n",
    "    return s\n",
    "\n",
    "def explode_date_range(begin_date_in: str,end_date_in: str,interval_unit_in: str,interval_in: int):\n",
    "    logging.info('explode_date_range-start')\n",
    "    logging.debug('begin_date_in: %s',begin_date_in)\n",
    "    logging.debug('end_date_in: %s',end_date_in)\n",
    "    begin_date = dateutil.parser.parse(begin_date_in)\n",
    "    end_date = dateutil.parser.parse(end_date_in)\n",
    "\n",
    "    begin_current=begin_date\n",
    "    result = []\n",
    "    \n",
    "    while begin_current < end_date:\n",
    "        end_current = begin_current+timedelta(**{interval_unit_in: interval_in})-timedelta(seconds=1)\n",
    "        result.append ([begin_current,end_current])\n",
    "        begin_current=end_current+timedelta(seconds=1)\n",
    "    logging.info('explode_date_range-end')\n",
    "    return result\n",
    "\n",
    "def write_search_partitions(date_array_in):\n",
    "    logging.info('write_search_partitions-start')\n",
    "    index_list=config.get('search', 'indexes').split(\",\")\n",
    "    \n",
    "    result_list = []\n",
    "    i=1\n",
    "    for index_entry in index_list:\n",
    "        for entry in date_array_in:\n",
    "            search_partition={'id':i,'index':index_entry,\n",
    "                'earliest': entry[0].isoformat(),'latest':entry[1].isoformat(),\n",
    "                #'earliest': entry[0].strftime(\"%m/%d/%Y:%H:%M:%S\"),'latest':entry[1].strftime(\"%m/%d/%Y:%H:%M:%S\"),\n",
    "                             'status':'not started','pid':''}\n",
    "            result_list.append(search_partition)\n",
    "            i+=1\n",
    "    \n",
    "    search_partitions=json.dumps(result_list, indent=2, sort_keys=True)\n",
    "    #print(search_partitions)      \n",
    "    \n",
    "    \n",
    "    with open(\"partitions.info\", \"w\") as outfile:\n",
    "        outfile.write(search_partitions)\n",
    "        \n",
    "    logging.info('write_search_partitions-end')\n",
    "\n",
    "def update_partition_status(partition_file_in,partition_in,status_in):\n",
    "    with open(partition_file_in,'r+') as f:\n",
    "        portalocker.lock(f, portalocker.LOCK_EX)\n",
    "        data = json.load(f)\n",
    "        search_partition_out=None\n",
    "        for search_partition in data:\n",
    "            if search_partition[\"id\"]==partition_in[\"id\"]:\n",
    "                #pprint(search_partition[\"earliest\"])\n",
    "                search_partition[\"status\"]=status_in\n",
    "                #return search_partition\n",
    "                break\n",
    "            \n",
    "        json_object = json.dumps(data, indent=4)\n",
    "        f.seek(0)\n",
    "        f.write(json_object)\n",
    "        f.truncate()\n",
    "    \n",
    "def get_search_partition(partition_file_in):\n",
    "    with open(partition_file_in,'r+') as f:\n",
    "        portalocker.lock(f, portalocker.LOCK_EX)\n",
    "        data = json.load(f)\n",
    "        search_partition_out=None\n",
    "        for search_partition in data:\n",
    "            if search_partition[\"status\"]=='not started':\n",
    "                #pprint(search_partition[\"earliest\"])\n",
    "                search_partition[\"status\"]='assigned'\n",
    "                search_partition[\"pid\"]=os.getpid()\n",
    "                search_partition_out=search_partition\n",
    "                #return search_partition\n",
    "                break\n",
    "                #pid = os.getpid()\n",
    "        #with open(\"sample.json\", \"w\") as outfile:\n",
    "        #outfile.write(json_object)\n",
    "        json_object = json.dumps(data, indent=4)\n",
    "        f.seek(0)\n",
    "        f.write(json_object)\n",
    "        f.truncate()\n",
    "        return search_partition_out\n",
    "        #f.write(json_object)\n",
    "        #pprint(data)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def search_export(service_in,search_in,partition_in):\n",
    "    logging.info('search_export-start')\n",
    "    \n",
    "\n",
    "    logging.debug(service_in)\n",
    "    kwargs_export = {\"search_mode\": \"normal\",\n",
    "                     'earliest_time': partition_in[\"earliest\"],\n",
    "                     'latest_time': partition_in[\"latest\"],\n",
    "                     \"output_mode\": \"json\"}\n",
    "    job = service_in.jobs.export(search_in, **kwargs_export)\n",
    "\n",
    "    logging.info(\"search_export-end\")\n",
    "    return job\n",
    "        \n",
    "def dispatch_searches(partition_file_in):\n",
    "    logging.info('dispatch_searches-start')\n",
    "\n",
    "    while True:\n",
    "        partition_out=get_search_partition('partitions.info')\n",
    "\n",
    "        if partition_out is not None:\n",
    "            search_string=build_search_string(partition_out)\n",
    "            service=connect()\n",
    "            job=search_export(service,search_string,partition_out)\n",
    "            #print_results(job)\n",
    "            write_results(job,partition_out)\n",
    "            update_partition_status(partition_file_in,partition_out,'completed')\n",
    "            #job.cancel()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    logging.info('dispatch_searches-end')\n",
    "\n",
    "        \n",
    "def write_results(job_in,partition_in):\n",
    "    logging.info('write_results-start')\n",
    "    earliest=partition_in[\"earliest\"]\n",
    "    earliest=re.sub(\"[/:]\", \"-\", earliest)\n",
    "\n",
    "    output_file=config.get('export', 'directory')+'/'+partition_in[\"index\"]+\"_\"+earliest+\".json\"\n",
    "    f = open(output_file, \"w\")\n",
    "    reader = results.JSONResultsReader(job_in)\n",
    "    for result in reader:\n",
    "        if isinstance(result, dict):\n",
    "            print(result,file=f)\n",
    "        elif isinstance(result, results.Message):\n",
    "            # Diagnostic messages may be returned in the results\n",
    "            print(result,file=f)\n",
    "    \n",
    "    f.close()\n",
    "    logging.info('write_results-end')\n",
    "\n",
    "        \n",
    "def connect():\n",
    "    try:\n",
    "        logging.info('connect-start')\n",
    "        logging.debug('SPLUNK_HOST: %s',config.get('splunk', 'SPLUNK_HOST'))\n",
    "        logging.debug('SPLUNK_PORT: %s',config.get('splunk', 'SPLUNK_PORT'))\n",
    "        logging.debug('SPLUNK_AUTH_TOKEN: %s',config.get('splunk', 'SPLUNK_AUTH_TOKEN'))\n",
    "       \n",
    "        service = client.connect(\n",
    "            host=config.get('splunk', 'SPLUNK_HOST'),\n",
    "            port=config.get('splunk', 'SPLUNK_PORT'),\n",
    "        #    username=USERNAME,\n",
    "        #    password=PASSWORD)\n",
    "            splunkToken=config.get('splunk', 'SPLUNK_AUTH_TOKEN'),\n",
    "            autologin=True)\n",
    "        logging.debug(service)\n",
    "        logging.info('connect-successful')\n",
    "        logging.info('connect-end')\n",
    "        return service\n",
    "    except:\n",
    "        logging.error('connect-failed')\n",
    "\n",
    "def main():\n",
    "    partition_file='partitions.info'\n",
    "    load_config()\n",
    "    service=connect()\n",
    "    logging.debug(service)\n",
    "    date_array=explode_date_range(config.get('search', 'begin_date'),config.get('search', 'end_date'),\n",
    "                              config.get('export', 'parition_units'),int(config.get('export', 'partition_interval')))\n",
    "    write_search_partitions(date_array)\n",
    "    create_output_dir()\n",
    "    #get_search_partition(partition_file)\n",
    "    dispatch_searches(partition_file)\n",
    "    #job.cancel()\n",
    "    #print_results2(job)\n",
    "    #write_results(job)\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a776562-3c7e-4518-8892-8aae20a731e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(partition_file,'r+') as f:\n",
    "    portalocker.lock(f, portalocker.LOCK_EX)\n",
    "    data = json.load(f)\n",
    "    search_partition_out=''\n",
    "    for search_partition in data:\n",
    "        if search_partition[\"status\"]=='not started':\n",
    "            pprint(search_partition[\"earliest\"])\n",
    "            search_partition[\"status\"]='assigned'\n",
    "            search_partition[\"pid\"]=os.getpid()\n",
    "            search_partition_out=search_partition\n",
    "            #return search_partition\n",
    "            break\n",
    "            #pid = os.getpid()\n",
    "    #with open(\"sample.json\", \"w\") as outfile:\n",
    "    #outfile.write(json_object)\n",
    "    json_object = json.dumps(data, indent=4)\n",
    "    f.seek(0)\n",
    "    f.write(json_object)\n",
    "    f.truncate()\n",
    "    #f.write(json_object)\n",
    "    pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90e863-5209-4cf4-a516-ef9ca3fb0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_file='partitions.info'\n",
    "f = open(partition_file)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3f782-d5fc-42fb-b5ca-bbc51308a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search_string():\n",
    "    logging.info('build_search_string-start')\n",
    "    logging.debug('indexes: %s',config.get('search', 'indexes'))\n",
    "    \n",
    "    index_list=config.get('search', 'indexes').split()\n",
    "    \n",
    "    logging.debug('extra: %s',config.get('search', 'extra'))\n",
    "    s='search index='+config.get('search', 'indexes')\n",
    "    s+=' '+config.get('search', 'extra')\n",
    "    \n",
    "    logging.debug('s: %s',s)\n",
    "    logging.info('build_search_string-end')\n",
    "    return s\n",
    "\n",
    "search_string=build_search_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145fb52-0b66-4552-8fe2-ba9b055ec5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partition_file='partitions.info'\n",
    "\n",
    "def search_export(service_in,search_in,partition_in):\n",
    "    logging.info('search_export-start')\n",
    "    \n",
    "\n",
    "    logging.debug(service_in)\n",
    "    kwargs_export = {\"search_mode\": \"normal\",\n",
    "                     'earliest_time': partition_in[\"earliest\"],\n",
    "                     'latest_time': partition_in[\"latest\"],\n",
    "                     \"output_mode\": \"json\"}\n",
    "    job = service_in.jobs.export(search_in, **kwargs_export)\n",
    "\n",
    "    logging.info(\"search_export-end\")\n",
    "    return job\n",
    "\n",
    "\n",
    "def update_partition_status(partition_file_in,partition_in,status_in):\n",
    "    with open(partition_file_in,'r+') as f:\n",
    "        portalocker.lock(f, portalocker.LOCK_EX)\n",
    "        data = json.load(f)\n",
    "        search_partition_out=None\n",
    "        for search_partition in data:\n",
    "            if search_partition[\"id\"]==partition_in[\"id\"]:\n",
    "                #pprint(search_partition[\"earliest\"])\n",
    "                search_partition[\"status\"]=status_in\n",
    "                #return search_partition\n",
    "                break\n",
    "            \n",
    "        json_object = json.dumps(data, indent=4)\n",
    "        f.seek(0)\n",
    "        f.write(json_object)\n",
    "        f.truncate()\n",
    "\n",
    "        \n",
    "\n",
    "def build_search_string(partition_in):\n",
    "    logging.info('build_search_string-start')\n",
    "    logging.debug('indexes: %s',config.get('search', 'indexes'))\n",
    "    logging.debug('extra: %s',config.get('search', 'extra'))\n",
    "    s='search index='+partition_in[\"index\"]\n",
    "    #s+=' earliest='+partition_in[\"earliest\"]\n",
    "    #s+=' latest='+partition_in[\"latest\"]\n",
    "    s+=' '+config.get('search', 'extra')\n",
    "    \n",
    "    logging.info('s: %s',s)\n",
    "    logging.info('build_search_string-end')\n",
    "    return s\n",
    "\n",
    "\n",
    "def write_results(job_in,partition_in):\n",
    "    logging.info('write_results-start')\n",
    "    earliest=partition_in[\"earliest\"]\n",
    "    earliest=re.sub(\"[/:]\", \"-\", earliest)\n",
    "\n",
    "    output_file=config.get('export', 'directory')+'/'+partition_in[\"index\"]+\"_\"+earliest+\".json\"\n",
    "    f = open(output_file, \"w\")\n",
    "    reader = results.JSONResultsReader(job_in)\n",
    "    for result in reader:\n",
    "        if isinstance(result, dict):\n",
    "            print(result,file=f)\n",
    "        elif isinstance(result, results.Message):\n",
    "            # Diagnostic messages may be returned in the results\n",
    "            print(result,file=f)\n",
    "    \n",
    "    #for result in results.JSONResultsReader(job_in.results(output_mode='json')):\n",
    "        #print(result)\n",
    "     #   print(result, file=f)\n",
    "     #   f.write(result.value)\n",
    "\n",
    "#    f.write(job_in)\n",
    "    f.close()\n",
    "    logging.info('write_results-end')\n",
    "\n",
    "def dispatch_searches(partition_file_in):\n",
    "    logging.info('dispatch_searches-start')\n",
    "    #search_partition=get_search_partition(partition_file)\n",
    "#    logging.info('\n",
    "    while True:\n",
    "        partition_out=get_search_partition('partitions.info')\n",
    "\n",
    "        if partition_out is not None:\n",
    "            print(\"Search it\")\n",
    "            print(partition_out[\"earliest\"])\n",
    "            search_string=build_search_string(partition_out)\n",
    "            service=connect()\n",
    "            job=search_export(service,search_string,partition_out)\n",
    "            #print_results(job)\n",
    "            write_results(job,partition_out)\n",
    "            update_partition_status(partition_file_in,partition_out,'completed')\n",
    "            #job.cancel()\n",
    "        else:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    logging.info('dispatch_searches-end')\n",
    "    \n",
    "def create_output_dir():\n",
    "    path=config.get('export', 'directory')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "\n",
    "load_config()\n",
    "service=connect()\n",
    "logging.debug(service)\n",
    "date_array=explode_date_range(config.get('search', 'begin_date'),config.get('search', 'end_date'),\n",
    "                          config.get('export', 'parition_units'),int(config.get('export', 'partition_interval')))\n",
    "write_search_partitions(date_array)\n",
    "create_output_dir()\n",
    "#get_search_partition(partition_file)\n",
    "dispatch_searches(partition_file)\n",
    "\n",
    "#job=search(service,search_string)\n",
    "#print_results(job)\n",
    "#job.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d36051-285e-4ae3-a4a9-7d8451e4298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_partition(partition_file_in):\n",
    "    with open(partition_file_in,'r+') as f:\n",
    "        portalocker.lock(f, portalocker.LOCK_EX)\n",
    "        data = json.load(f)\n",
    "        search_partition_out=None\n",
    "        for search_partition in data:\n",
    "            if search_partition[\"status\"]=='not started':\n",
    "                #pprint(search_partition[\"earliest\"])\n",
    "                search_partition[\"status\"]='assigned'\n",
    "                search_partition[\"pid\"]=os.getpid()\n",
    "                search_partition_out=search_partition\n",
    "                #return search_partition\n",
    "                break\n",
    "                #pid = os.getpid()\n",
    "        #with open(\"sample.json\", \"w\") as outfile:\n",
    "        #outfile.write(json_object)\n",
    "        json_object = json.dumps(data, indent=4)\n",
    "        f.seek(0)\n",
    "        f.write(json_object)\n",
    "        f.truncate()\n",
    "        return search_partition_out\n",
    "        #f.write(json_object)\n",
    "        #pprint(data)\n",
    "        \n",
    "partition_out=get_search_partition('partitions.info')\n",
    "\n",
    "if partition_out is not None:\n",
    "    print(\"Search it\")\n",
    "else:\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e908f91-b90d-4f01-8428-8a5083880068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "\n",
    "str = \"06/26/2022:00:00:00\"\n",
    "print(re.sub(\"[/:]\", \"-\", str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9148b2-f30e-4642-a57e-6d6954b7e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_array=explode_date_range(config.get('search', 'begin_date'),config.get('search', 'end_date'),\n",
    "                          config.get('export', 'parition_units'),int(config.get('export', 'partition_interval')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fed40a-e1e8-4e05-bb27-8a5b74c32535",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.get('search', 'begin_date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572099c-d562-401c-8d7c-953a740c8237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
