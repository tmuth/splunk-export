{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74848c1b-173b-45e9-bee3-4f089d168f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "import dateutil.parser\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import portalocker\n",
    "import logging\n",
    "#from splunk_http_event_collector import http_event_collector\n",
    "import timeout_decorator\n",
    "import configparser\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import splunklib.client as client\n",
    "import splunklib.results as results\n",
    "import sys\n",
    "from time import sleep\n",
    "\n",
    "#if len(sys.argv) < 2:\n",
    "#    print \"Pass the name of a config file as argument 1\"\n",
    "#    exit(1)\n",
    "\n",
    "#if not os.path.exists(sys.argv[1]):\n",
    "#    print \"Cannot find the configuration file: \"+sys.argv[1]\n",
    "#    exit(1)\n",
    "config_file='export1.conf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883934aa-dc69-4731-8957-354ba55898ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                    datefmt='%m-%d %H:%M:%S')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers[0].stream = sys.stdout\n",
    "logger.setLevel(logging.INFO) # DEBUG,INFO,WARNING,ERROR,CRITICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d4980-2069-406d-87ad-efcbd6375c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a6e91a-c606-421c-8721-82f51ee3b52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    global config\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "def create_output_dir():\n",
    "    path=config.get('export', 'directory')\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def build_search_string(partition_in):\n",
    "    logging.info('build_search_string-start')\n",
    "    logging.debug('indexes: %s',config.get('search', 'indexes'))\n",
    "    logging.debug('extra: %s',config.get('search', 'extra'))\n",
    "    s='search index='+partition_in[\"index\"]\n",
    "    #s+=' earliest='+partition_in[\"earliest\"]\n",
    "    #s+=' latest='+partition_in[\"latest\"]\n",
    "    s+=' '+config.get('search', 'extra')\n",
    "    \n",
    "    logging.debug('s: %s',s)\n",
    "    logging.info('build_search_string-end')\n",
    "    return s\n",
    "\n",
    "def explode_date_range(begin_date_in: str,end_date_in: str,interval_unit_in: str,interval_in: int):\n",
    "    logging.info('explode_date_range-start')\n",
    "    logging.debug('begin_date_in: %s',begin_date_in)\n",
    "    logging.debug('end_date_in: %s',end_date_in)\n",
    "    begin_date = dateutil.parser.parse(begin_date_in)\n",
    "    end_date = dateutil.parser.parse(end_date_in)\n",
    "\n",
    "    begin_current=begin_date\n",
    "    result = []\n",
    "    \n",
    "    while begin_current < end_date:\n",
    "        end_current = begin_current+timedelta(**{interval_unit_in: interval_in})-timedelta(seconds=1)\n",
    "        result.append ([begin_current,end_current])\n",
    "        begin_current=end_current+timedelta(seconds=1)\n",
    "    logging.info('explode_date_range-end')\n",
    "    return result\n",
    "\n",
    "def write_search_partitions(date_array_in):\n",
    "    logging.info('write_search_partitions-start')\n",
    "    index_list=config.get('search', 'indexes').split(\",\")\n",
    "    \n",
    "    \n",
    "    json_data = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    result_list = []\n",
    "    i=0\n",
    "    for index_entry in index_list:\n",
    "        for entry in date_array_in:\n",
    "            i+=1\n",
    "            search_partition={'id':i,'index':index_entry,\n",
    "                'earliest': entry[0].isoformat(),'latest':entry[1].isoformat(),\n",
    "                #'earliest': entry[0].strftime(\"%m/%d/%Y:%H:%M:%S\"),'latest':entry[1].strftime(\"%m/%d/%Y:%H:%M:%S\"),\n",
    "                             'status':'not started','pid':''}\n",
    "            result_list.append(search_partition)\n",
    "    \n",
    "    summary_data={'partition_count': i,\n",
    "                  'complete_count': 0,\n",
    "                  'status': 'starting',\n",
    "                  'start_time': datetime.now().isoformat(),\n",
    "                  'end_time': '',\n",
    "                  'total_seconds': ''}\n",
    "    \n",
    "    json_data['summary_data']=summary_data\n",
    "    json_data['partitions']=result_list\n",
    "    \n",
    "    search_partitions=json.dumps(json_data, indent=2, sort_keys=False)\n",
    "    #print(search_partitions)      \n",
    "    \n",
    "    \n",
    "    with open(\"partitions.info\", \"w\") as outfile:\n",
    "        outfile.write(search_partitions)\n",
    "        \n",
    "    logging.info('write_search_partitions-end')\n",
    "\n",
    "def update_partition_status(partition_file_in,partition_in,status_in):\n",
    "    with open(partition_file_in,'r+') as f:\n",
    "        portalocker.lock(f, portalocker.LOCK_EX)\n",
    "        data = json.load(f)\n",
    "        search_partition_out=None\n",
    "        for search_partition in data[\"partitions\"]:\n",
    "            if search_partition[\"id\"]==partition_in[\"id\"]:\n",
    "                #pprint(search_partition[\"earliest\"])\n",
    "                search_partition[\"status\"]=status_in\n",
    "                #return search_partition\n",
    "                break\n",
    "                \n",
    "        if status_in == 'completed':\n",
    "            data[\"summary_data\"][\"complete_count\"]=int(data[\"summary_data\"][\"complete_count\"])+1\n",
    "            \n",
    "        json_object = json.dumps(data, indent=4)\n",
    "        f.seek(0)\n",
    "        f.write(json_object)\n",
    "        f.truncate()\n",
    "\n",
    "def finalize_partition_status(partition_file_in):\n",
    "    with open(partition_file_in,'r+') as f:\n",
    "        portalocker.lock(f, portalocker.LOCK_EX)\n",
    "        data = json.load(f)\n",
    "        data[\"summary_data\"][\"end_time\"]=datetime.now().isoformat()\n",
    "        date_start=datetime.fromisoformat(data[\"summary_data\"][\"start_time\"])\n",
    "        date_end=datetime.fromisoformat(data[\"summary_data\"][\"end_time\"])\n",
    "        data[\"summary_data\"][\"total_seconds\"] = round((date_end-date_start).total_seconds(),1)\n",
    "        json_object = json.dumps(data, indent=4)\n",
    "        f.seek(0)\n",
    "        f.write(json_object)\n",
    "        f.truncate()\n",
    "        \n",
    "        \n",
    "def get_search_partition(partition_file_in):\n",
    "    with open(partition_file_in,'r+') as f:\n",
    "        portalocker.lock(f, portalocker.LOCK_EX)\n",
    "        data = json.load(f)\n",
    "        search_partition_out=None\n",
    "        for search_partition in data[\"partitions\"]:\n",
    "            if search_partition[\"status\"]=='not started':\n",
    "                #pprint(search_partition[\"earliest\"])\n",
    "                search_partition[\"status\"]='assigned'\n",
    "                search_partition[\"pid\"]=os.getpid()\n",
    "                search_partition_out=search_partition\n",
    "                #return search_partition\n",
    "                break\n",
    "                #pid = os.getpid()\n",
    "        #with open(\"sample.json\", \"w\") as outfile:\n",
    "        #outfile.write(json_object)\n",
    "        json_object = json.dumps(data, indent=4)\n",
    "        f.seek(0)\n",
    "        f.write(json_object)\n",
    "        f.truncate()\n",
    "        return search_partition_out\n",
    "        #f.write(json_object)\n",
    "        #pprint(data)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def search_export(service_in,search_in,partition_in):\n",
    "    logging.info('search_export-start')\n",
    "\n",
    "    logging.debug(service_in)\n",
    "    kwargs_export = {\"search_mode\": \"normal\",\n",
    "                     'earliest_time': partition_in[\"earliest\"],\n",
    "                     'latest_time': partition_in[\"latest\"],\n",
    "                     \"output_mode\": \"json\"}\n",
    "    job = service_in.jobs.export(search_in, **kwargs_export)\n",
    "\n",
    "    logging.info(\"search_export-end\")\n",
    "    return job\n",
    "        \n",
    "def dispatch_searches(partition_file_in):\n",
    "    logging.info('dispatch_searches-start')\n",
    "\n",
    "    while True:\n",
    "        partition_out=get_search_partition('partitions.info')\n",
    "\n",
    "        if partition_out is not None:\n",
    "            search_string=build_search_string(partition_out)\n",
    "            service=connect()\n",
    "            job=search_export(service,search_string,partition_out)\n",
    "            #print_results(job)\n",
    "            write_results(job,partition_out)\n",
    "            update_partition_status(partition_file_in,partition_out,'completed')\n",
    "            #job.cancel()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    logging.info('dispatch_searches-end')\n",
    "\n",
    "        \n",
    "def write_results(job_in,partition_in):\n",
    "    logging.info('write_results-start')\n",
    "    earliest=partition_in[\"earliest\"]\n",
    "    earliest=re.sub(\"[/:]\", \"-\", earliest)\n",
    "\n",
    "    output_file=config.get('export', 'directory')+'/'+partition_in[\"index\"]+\"_\"+earliest+\".json\"\n",
    "    f = open(output_file, \"w\")\n",
    "    reader = results.JSONResultsReader(job_in)\n",
    "    for result in reader:\n",
    "        if isinstance(result, dict):\n",
    "            print(result,file=f)\n",
    "        elif isinstance(result, results.Message):\n",
    "            # Diagnostic messages may be returned in the results\n",
    "            print(result,file=f)\n",
    "    \n",
    "    f.close()\n",
    "    logging.info('write_results-end')\n",
    "\n",
    "        \n",
    "def connect():\n",
    "    try:\n",
    "        logging.info('connect-start')\n",
    "        logging.debug('SPLUNK_HOST: %s',config.get('splunk', 'SPLUNK_HOST'))\n",
    "        logging.debug('SPLUNK_PORT: %s',config.get('splunk', 'SPLUNK_PORT'))\n",
    "        logging.debug('SPLUNK_AUTH_TOKEN: %s',config.get('splunk', 'SPLUNK_AUTH_TOKEN'))\n",
    "       \n",
    "        service = client.connect(\n",
    "            host=config.get('splunk', 'SPLUNK_HOST'),\n",
    "            port=config.get('splunk', 'SPLUNK_PORT'),\n",
    "        #    username=USERNAME,\n",
    "        #    password=PASSWORD)\n",
    "            splunkToken=config.get('splunk', 'SPLUNK_AUTH_TOKEN'),\n",
    "            autologin=True)\n",
    "        logging.debug(service)\n",
    "        logging.info('connect-successful')\n",
    "        logging.info('connect-end')\n",
    "        return service\n",
    "    except:\n",
    "        logging.error('connect-failed')\n",
    "\n",
    "def main():\n",
    "    partition_file='partitions.info'\n",
    "    load_config()\n",
    "    service=connect()\n",
    "    logging.debug(service)\n",
    "    date_array=explode_date_range(config.get('search', 'begin_date'),config.get('search', 'end_date'),\n",
    "                              config.get('export', 'parition_units'),int(config.get('export', 'partition_interval')))\n",
    "    write_search_partitions(date_array)\n",
    "    create_output_dir()\n",
    "    #get_search_partition(partition_file)\n",
    "    dispatch_searches(partition_file)\n",
    "    finalize_partition_status(partition_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6108100-c908-4579-b7f9-6b63aa8220a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "partition_file='partitions.info'\n",
    "load_config()\n",
    "service=connect()\n",
    "logging.debug(service)\n",
    "date_array=explode_date_range(config.get('search', 'begin_date'),config.get('search', 'end_date'),\n",
    "                          config.get('export', 'parition_units'),int(config.get('export', 'partition_interval')))\n",
    "write_search_partitions(date_array)\n",
    "create_output_dir()\n",
    "#get_search_partition(partition_file)\n",
    "process_list = []\n",
    "for i in range(4):\n",
    "    p =  multiprocessing.Process(target= dispatch_searches,args=(partition_file))\n",
    "    p.start()\n",
    "    process_list.append(p)\n",
    "\n",
    "for process in process_list:\n",
    "    process.join()\n",
    "\n",
    "#dispatch_searches(partition_file)\n",
    "finalize_partition_status(partition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00de07-cb88-4e50-8233-726a314eaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "with mp.Pool(5) as pool:\n",
    "    print(pool.map(f, [1, 2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192fbe0-1efd-4ab2-91aa-d8e1b8fff6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\", line 368, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'single' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "class OtherClass:\n",
    "  def run(self, sentence, graph):\n",
    "    return False\n",
    "\n",
    "\n",
    "def single(params):\n",
    "    other = OtherClass()\n",
    "    sentences, graph = params\n",
    "    return [other.run(sentence, graph) for sentence in sentences]\n",
    "\n",
    "class SomeClass:\n",
    "   def __init__(self):\n",
    "       self.sentences = [[\"Some string\"]]\n",
    "       self.graphs = [\"string\"]\n",
    "\n",
    "   def some_method(self):\n",
    "      return list(pool.map(single, zip(self.sentences, self.graphs)))\n",
    "\n",
    "if __name__ == '__main__':  # <- prevent RuntimeError for 'spawn'\n",
    "    # and 'forkserver' start_methods\n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count() - 1) as pool:\n",
    "        print(SomeClass().some_method())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dacb3a-72f7-4d0a-bf28-e5fb6568e775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
